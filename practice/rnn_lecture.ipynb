{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_lecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOEUcuKYQl4vNT3uxHe265V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/4sunshine/dl-unn.github.io/blob/master/practice/rnn_lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6_g4aUOrUVw"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Input, SimpleRNN, Dense, TimeDistributed, Softmax, GRU\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emyvy8ZarW7-",
        "outputId": "f9c0fd27-e08b-4461-8abc-48fa3094691f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# MAIN GOAL OF THIS TASK IS TO GET A STABLE PREDICTION OF 'HELLO'\n",
        "# FROM SINGLE SYMBOL 'H' MODEL INPUT\n",
        "# ANY YOUR IDEAS ARE WELCOME\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "ALL_TEXTS = ['HELLO']\n",
        "\n",
        "vocab = sorted({char for word in ALL_TEXTS for char in word})\n",
        "print(vocab)\n",
        "\n",
        "id2char = {i: s for i, s in enumerate(vocab)}\n",
        "char2id = {s: i for i, s in enumerate(vocab)}\n",
        "\n",
        "def onehot(char):\n",
        "  # NP.ARRAY [VOCAB_SIZE] ex. [0, 0, 0, 1]\n",
        "  return np.array([1 if char2id[char]==i else 0 for i in range(len(vocab))])\n",
        "  \n",
        "def text2hots(text):\n",
        "  # NP.ARRAY [NUM LETTERS, VOCAB_SIZE] ex. [[1, 0, 0, 0], [0, 1, 0, 0]]\n",
        "  return np.array([onehot(char) for char in text], dtype=np.float64)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['E', 'H', 'L', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3KNJVus6Pqw"
      },
      "source": [
        "def build_model(vocab_size, HIDDEN_SIZE=3):\n",
        "  # YOU CAN PLAY WITH HIDDEN_SIZE\n",
        "  # OR USE LSTM instead of SimpleRNN\n",
        "  # OR USE different ACTIVATIONS, play with BIAS\n",
        "  # USE EMBEDDING LAYER BEFORE RNN\n",
        "  # PUT THE SECOND RNN HERE\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  # RNN_INPUT: [BATCH_SIZE=1, NUM_LETTERS (ARBITRARY), VOCABULARY SIZE]\n",
        "  model.add(SimpleRNN(HIDDEN_SIZE, input_shape=(None, vocab_size),\n",
        "                      return_sequences=True,\n",
        "                      use_bias=True,\n",
        "                      activation='tanh'))\n",
        "  # FULLY CONNECTED LAYER\n",
        "  # RETURNS LOGITS TENSOR [BATCH_SIZE=1, NUM_LETTERS, VOCABULARY_SIZE]\n",
        "  model.add(Dense(vocab_size))\n",
        "  model.compile()\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "def generate_text(model, start_symbol):\n",
        "  text = start_symbol\n",
        "  hot = text2hots(start_symbol)\n",
        "\n",
        "  # CONVERT TO SHAPE [1 (BATCH_SIZE), NUM_LETTERS (TIMESTEPS),\n",
        "  # X_VECTOR_SIZE (VOCAB_SIZE)]\n",
        "  hot = np.expand_dims(hot, 0)\n",
        "\n",
        "  model.reset_states()\n",
        "  for _ in range(4):\n",
        "    prediction = model(hot)\n",
        "\n",
        "    # REMOVE BATCH DIMENSION\n",
        "    prediction = tf.squeeze(prediction, 0)\n",
        "\n",
        "    # ID OF LETTER IN VOCABULARY\n",
        "    predicted_id = tf.math.argmax(prediction, axis=-1).numpy()[0]\n",
        "\n",
        "    predicted_symbol = id2char[predicted_id]\n",
        "    text += predicted_symbol\n",
        "\n",
        "    # ONE-HOT ENCODE LAST PREDICTED SYMBOL\n",
        "    hot = text2hots(predicted_symbol)\n",
        "\n",
        "    # CONVERT TO SHAPE [1 (BATCH_SIZE), NUM_LETTERS (TIMESTEPS),\n",
        "    # X_VECTOR_SIZE (VOCAB_SIZE)]\n",
        "    hot = np.expand_dims(hot, 0)\n",
        "  return text\n",
        "\n",
        "# YOU CAN CREATE YOUR OWN TRAINING PIPELINE SPECIAL FOR THIS TASK\n",
        "# e.g. PREDICT TARGET LETTER IN FOR LOOP\n",
        "@tf.function\n",
        "def train_step(model, opt, inp, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inp)\n",
        "\n",
        "        # CALCULATE PREDICTED SEQUENCE LOSS\n",
        "        loss = tf.reduce_mean(\n",
        "            tf.keras.losses.categorical_crossentropy(target,\n",
        "                                                     predictions,\n",
        "                                                     from_logits=True),\n",
        "                              axis=-1)\n",
        "\n",
        "    # CALCULATE GRADIENTS FROM LOSS FUNCTION        \n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    # APPLY GRADIENTS TO MODEL\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hYg25U20GRA",
        "outputId": "f3a18a4a-172d-441a-e7a4-61254913cf71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# OPTIMIZER\n",
        "# YOU CAN CHANGE OPTIMIZER AND PLAY WITH ITS LEARNING RATE\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
        "\n",
        "# OUR DATASET\n",
        "text_input = 'HELL'\n",
        "text_output = 'ELLO'\n",
        "\n",
        "# YOU CAN MODIFY DATASET WITH SHIFTED SEQUENCES LIKE\n",
        "# input 'ELLO', target 'LLOH'\n",
        "# input 'LLOH', target 'LOHE' etc.\n",
        "\n",
        "model = build_model(len(vocab))\n",
        "\n",
        "for epoch in range(50):\n",
        "  # CURRENTLY EPOCH CONSISTS ONLY OF 1 TRAINING EXAMPLE\n",
        "\n",
        "  # RESET INITIAL HIDDEN STATE\n",
        "  model.reset_states()\n",
        "  \n",
        "  # X AND Y TO ONE-HOT WITH SHAPE [1, NUM_LETTERS, LEN_VOCAB]\n",
        "  x = np.expand_dims(text2hots(text_input), 0)\n",
        "\n",
        "  y = np.expand_dims(text2hots(text_output), 0)\n",
        "\n",
        "  # TRAIN MODEL WITH ONE STEP\n",
        "  loss = train_step(model, optimizer, x, y)\n",
        "\n",
        "  # VALIDATE MODEL\n",
        "  if epoch % 2 == 0:\n",
        "    print(f'epoch: {epoch}, loss: {loss.numpy()}')\n",
        "    text = generate_text(model, 'H')\n",
        "    print(text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn (SimpleRNN)       (None, None, 3)           24        \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, None, 4)           16        \n",
            "=================================================================\n",
            "Total params: 40\n",
            "Trainable params: 40\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "loss: [1.8906832]\n",
            "HLEEE\n",
            "loss: [1.4843457]\n",
            "HLELE\n",
            "loss: [1.2807729]\n",
            "HLLLL\n",
            "loss: [1.0895083]\n",
            "HLLLL\n",
            "loss: [0.9175042]\n",
            "HLOLO\n",
            "loss: [0.79906666]\n",
            "HLOLO\n",
            "loss: [0.69465196]\n",
            "HLOLO\n",
            "loss: [0.6000497]\n",
            "HLLLL\n",
            "loss: [0.51886374]\n",
            "HLLLL\n",
            "loss: [0.45230272]\n",
            "HELLL\n",
            "loss: [0.39407352]\n",
            "HELLL\n",
            "loss: [0.3351027]\n",
            "HELLL\n",
            "loss: [0.27923965]\n",
            "HELLL\n",
            "loss: [0.23096496]\n",
            "HELLL\n",
            "loss: [0.19076142]\n",
            "HELLL\n",
            "loss: [0.15795903]\n",
            "HELLL\n",
            "loss: [0.13147989]\n",
            "HELLL\n",
            "loss: [0.11020262]\n",
            "HELLL\n",
            "loss: [0.09313775]\n",
            "HELOE\n",
            "loss: [0.07945786]\n",
            "HELOE\n",
            "loss: [0.06847647]\n",
            "HELOE\n",
            "loss: [0.05962631]\n",
            "HELOE\n",
            "loss: [0.05244464]\n",
            "HELOE\n",
            "loss: [0.04656332]\n",
            "HELOE\n",
            "loss: [0.04169841]\n",
            "HELOE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9hT3Ryb0rzW"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}